{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Using matplotlib backend: TkAgg\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras import Input, Model\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "from callbacks import CallbackPlot, CallbackSaveLogs\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_processor import clean_data, build_datasets\n",
    "from data_processor import one_hot2char, process_input\n",
    "\n",
    "%matplotlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "from shutil import copyfile\n",
    "from datetime import datetime\n",
    "\n",
    "MAX_LEN_TITLE = 82\n",
    "MAX_LEN_TEXT = 390\n",
    "\n",
    "# enable memory growth to be able to work with GPU\n",
    "GPU = tf.config.experimental.get_visible_devices('GPU')[0]\n",
    "tf.config.experimental.set_memory_growth(GPU, enable=True)\n",
    "\n",
    "# set tensorflow to work with float64\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "# the new line character (\\n) is the 'end of sentence', therefore there is no need to add a '[STOP]' character\n",
    "vocab = '\\'kespw/br461 \\ncunt-\".x:j0ml,(qzgif)25a7o9vdyh83'\n",
    "vocab = list(vocab) + ['[START]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Using gradient clipping\n"
    }
   ],
   "source": [
    "\n",
    "config = {  # dictionary that contains the training set up. Will be saved as a JSON file\n",
    "    'DIM_VOCAB': len(vocab),\n",
    "    'DIM_LSTM_LAYER': 1024,\n",
    "    'LEARNING_RATE': 0.002,\n",
    "    'BATCH_SIZE': 8,\n",
    "    'EPOCHS': 40,\n",
    "    'SEED': 1,\n",
    "    'GRAD_VAL_CLIP': 0.5,\n",
    "    'GRAD_NORM_CLIP': 1,\n",
    "    'DECAY_AT_10_EPOCHS': 1/2,\n",
    "    'DROPOUT': 0.1,\n",
    "}\n",
    "tf.random.set_seed(config['SEED'])\n",
    "data = pd.read_csv('Complete Database.csv', index_col=0)\n",
    "\n",
    "data = clean_data(data)\n",
    "\n",
    "config['STEPS_PER_EPOCH'] = int((data.shape[0] - 3000) / config['BATCH_SIZE'])\n",
    "config['VALIDATION_SAMPLES'] = int(data.shape[0]) - (config['STEPS_PER_EPOCH'] * config['BATCH_SIZE'])\n",
    "config['VALIDATION_STEPS'] = int(np.floor(config['VALIDATION_SAMPLES'] / config['BATCH_SIZE']))\n",
    "\n",
    "# configures the learning rate to be decayed by the value specified at config['DECAY_AT_10_EPOCHS'] at each 10 epochs, but to that gradally at each epoch\n",
    "learning_rate = ExponentialDecay(initial_learning_rate=config['LEARNING_RATE'], \n",
    "                                decay_steps=config['STEPS_PER_EPOCH'], \n",
    "                                decay_rate=np.power(config['DECAY_AT_10_EPOCHS'], 1/10), \n",
    "                                staircase=True)\n",
    "\n",
    "\"\"\" config['STEPS_PER_EPOCH'] = 5\n",
    "config['VALIDATION_SAMPLES'] = 100\n",
    "config['VALIDATION_STEPS'] = 5\n",
    " \"\"\"\n",
    "training_dataset, validation_dataset = build_datasets(data, seed=config['SEED'], validation_samples=config['VALIDATION_SAMPLES'], batch=config['BATCH_SIZE'], vocab=vocab)\n",
    "\n",
    "#learning_rate = config['LEARNING_RATE']\n",
    "\n",
    "if 'GRAD_VAL_CLIP' in config:\n",
    "    optimizer = Adam(learning_rate=learning_rate, clipvalue=config['GRAD_VAL_CLIP'], clipnorm=config['GRAD_NORM_CLIP'])\n",
    "    print('Using gradient clipping')\n",
    "else:\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "config['OPTIMIZER'] = optimizer._name\n",
    "\n",
    "folder_path = 'Training Logs/Training'  # creates folder to save traning logs\n",
    "if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "with open(folder_path + '/config.json', 'w') as json_file: # saves the training configuration as a JSON file\n",
    "    json.dump(config, json_file, indent=4)\n",
    "\n",
    "with open(folder_path + '/vocab.json', 'w')  as json_file: # saves the vocab used as a JSON file\n",
    "    json.dump(vocab, json_file, indent=4)\n",
    "\n",
    "\n",
    "validation_steps = int(config['VALIDATION_SAMPLES'] / config['BATCH_SIZE'])\n",
    "\n",
    "loss_plot_settings = {'variables': {'loss': 'Training loss',\n",
    "                                        'val_loss': 'Validation loss'},\n",
    "                          'title': 'Losses',\n",
    "                          'ylabel': 'Epoch Loss'}\n",
    "\n",
    "last_5_plot_settings = {'variables': {'loss': 'Training loss',\n",
    "                                        'val_loss': 'Validation loss'},\n",
    "                          'title': 'Losses',\n",
    "                          'ylabel': 'Epoch Loss',\n",
    "                          'last_epochs': 5}\n",
    "\n",
    "plot_callback = CallbackPlot(folder_path=folder_path,\n",
    "                            plots_settings=[loss_plot_settings, last_5_plot_settings],\n",
    "                            title='Losses', share_x=False)\n",
    "\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=folder_path + '/trained_model.h5')\n",
    "\n",
    "csv_logger = CSVLogger(filename=folder_path + '/Training logs.csv', separator=',', append=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### BUILDS MODEL FROM SCRATCH ###########\n",
    "tf.keras.backend.clear_session() # destroys the current graph\n",
    "\n",
    "encoder_inputs = Input(shape=(None, config['DIM_VOCAB']), name='encoder_input')\n",
    "encoder_LSTM = LSTM(units = config['DIM_LSTM_LAYER'], return_sequences=False, return_state=True, name='encoder_LSTM', dropout=config['DROPOUT'])\n",
    "enc_output, enc_memory_state, enc_carry_state = encoder_LSTM(encoder_inputs)\n",
    "\n",
    "decoder_inputs = Input(shape=(None, config['DIM_VOCAB']), name='decoder_input')\n",
    "decoder_LSTM = LSTM(units=config['DIM_LSTM_LAYER'], return_sequences=True, return_state=True, name='decoder_LSTM', dropout=config['DROPOUT'])  # return_state must be set in order to retrieve the internal states in inference model later\n",
    "decoder_output, _, _ = decoder_LSTM(decoder_inputs, initial_state=[enc_memory_state, enc_carry_state])\n",
    "\n",
    "dense = Dense(units=config['DIM_VOCAB'], activation='softmax', name='output')\n",
    "dense_output = dense(decoder_output)\n",
    "\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=dense_output)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### LOADS MODEL FROM DISK ###########\n",
    "\n",
    "tf.keras.backend.clear_session() # destroys the current graph\n",
    "\n",
    "model_folder = 'Training Logs/Training Session - 27-Apr-20 -- 22-57-10'\n",
    "\n",
    "model = tf.keras.models.load_model(model_folder + '/trained_model.h5')\n",
    "\n",
    "copyfile(model_folder + '/Training logs.csv', folder_path + '/Training logs.csv') # makes a copy of the logs from previous training to the destination folder\n",
    "\n",
    "csv_logger = CSVLogger(filename=folder_path + '/Training logs.csv', separator=',', append=True)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train for 2251 steps, validate for 375 steps\nEpoch 1/40\n2251/2251 [==============================] - 2428s 1s/step - loss: 0.2185 - val_loss: 0.1345\nEpoch 2/40\n2251/2251 [==============================] - 2417s 1s/step - loss: 0.1413 - val_loss: 0.1126\nEpoch 3/40\n2251/2251 [==============================] - 2438s 1s/step - loss: 0.1192 - val_loss: 0.1013\nEpoch 4/40\n2251/2251 [==============================] - 2456s 1s/step - loss: 0.1128 - val_loss: 0.0943\nEpoch 5/40\n2251/2251 [==============================] - 2450s 1s/step - loss: 0.1029 - val_loss: 0.0932\nEpoch 6/40\n2251/2251 [==============================] - 2450s 1s/step - loss: 0.0980 - val_loss: 0.0859\nEpoch 7/40\n2251/2251 [==============================] - 2465s 1s/step - loss: 0.0974 - val_loss: 0.0833\nEpoch 8/40\n2251/2251 [==============================] - 2461s 1s/step - loss: 0.0906 - val_loss: 0.0836\nEpoch 9/40\n2251/2251 [==============================] - 2457s 1s/step - loss: 0.0875 - val_loss: 0.0790\nEpoch 10/40\n2251/2251 [==============================] - 2480s 1s/step - loss: 0.0883 - val_loss: 0.0776\nEpoch 11/40\n2251/2251 [==============================] - 2492s 1s/step - loss: 0.0834 - val_loss: 0.0787\nEpoch 12/40\n2251/2251 [==============================] - 2463s 1s/step - loss: 0.0811 - val_loss: 0.0754\nEpoch 13/40\n   1/2251 [..............................] - ETA: 40:44 - loss: 0.1581"
    }
   ],
   "source": [
    "\n",
    "history = model.fit(x=training_dataset,\n",
    "                    epochs=config['EPOCHS'],\n",
    "                    steps_per_epoch=config['STEPS_PER_EPOCH'],\n",
    "                    callbacks=[plot_callback, csv_logger, model_checkpoint_callback],\n",
    "                    validation_data=validation_dataset,\n",
    "                    validation_steps=config['VALIDATION_STEPS'])\n",
    "                   \n",
    "\n",
    "'''\n",
    "model.save(folder_path + '/trained_model.h5', save_format='h5')\n",
    "model.save_weights(folder_path + '/trained_model_weights.h5')\n",
    "plot_model(model, to_file=folder_path + '/model_layout.png', show_shapes=True, show_layer_names=True, rankdir='LR')\n",
    "'''\n",
    "\n",
    "timestamp_end = datetime.now().strftime('%d-%b-%y -- %H:%M:%S')\n",
    "\n",
    "# renames the training folder with the end-of-training timestamp\n",
    "root, _ = os.path.split(folder_path)\n",
    "\n",
    "timestamp_end = timestamp_end.replace(':', '-')\n",
    "os.rename(folder_path, root + '/' + 'Training Session - ' + timestamp_end)\n",
    "\n",
    "print(\"Training Succesfully finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a dataframe that contains all the characters and their count in all the texts and titles\n",
    "import pandas as pd\n",
    "from data_processor import clean_data\n",
    "\n",
    "data = pd.read_csv('Complete Database.csv', index_col=0)\n",
    "\n",
    "data = clean_data(data)\n",
    "\n",
    "concatenated_text = data.Text.str.cat() + data.Title.str.cat()\n",
    "vocab = set(list(concatenated_text))\n",
    "\n",
    "char_count = []\n",
    "for char in vocab:\n",
    "    char_count.append([char, concatenated_text.count(char)])\n",
    "\n",
    "df_count = pd.DataFrame(char_count, columns=['Char', 'Count']) # must be 'Count' with capital 'c' to avoid conflict with function 'count' form pandas\n",
    "df_count.set_index('Char', inplace=True)\n",
    "df_count.sort_values(by='Count', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "47"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "\n",
    "model_folder1 = 'Training Logs/Training Session - 03-May-20 -- 04-00-16'\n",
    "model_folder2 = 'Training Logs/Training Session - 04-May-20 -- 04-41-34'\n",
    "with open(model_folder1 + '/config.json') as json_file:\n",
    "    config1 = json.load(json_file)\n",
    "with open(model_folder2 + '/config.json') as json_file:\n",
    "    config2 = json.load(json_file)\n",
    "\n",
    "config1 == config2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = 'Training Logs/Training'\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session() # destroys the current graph\n",
    "\n",
    "# loads trained model from H5 file. 't' stands for 'trained'\n",
    "t_model = tf.keras.models.load_model(model_folder + '/trained_model.h5')\n",
    "\n",
    "# loads model configuration from JSON file\n",
    "with open(model_folder + '/config.json') as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "# loads vocab from JSON file\n",
    "with open(model_folder + '/vocab.json') as json_file:\n",
    "    vocab = json.load(json_file)\n",
    "\n",
    "# encoder model. 't' stands for 'trained'\n",
    "t_enc_input = t_model.get_layer('encoder_input').input\n",
    "t_enc_LSTM = t_model.get_layer('encoder_LSTM')\n",
    "[_, t_enc_memory_state, t_enc_carry_state] = t_enc_LSTM(t_enc_input)\n",
    "t_enc_model = Model(inputs=t_enc_input,\n",
    "                    outputs=[t_enc_memory_state, t_enc_carry_state])\n",
    "\n",
    "# decoder inputs\n",
    "t_dec_input = Input(shape=(None, config['DIM_VOCAB']), name='decoder_input')\n",
    "t_dec_memory_state_input = Input(shape=(config['DIM_LSTM_LAYER']), name='decoder_memory_state_input')\n",
    "t_dec_carry_state_input = Input(shape=(config['DIM_LSTM_LAYER']), name='decoder_carry_state_input')\n",
    "\n",
    "# decoder model\n",
    "t_dec_LSTM = t_model.get_layer('decoder_LSTM')\n",
    "t_dec_output, t_dec_memory_state_output, t_dec_carry_state_output = t_dec_LSTM(t_dec_input, initial_state=[t_dec_memory_state_input, t_dec_carry_state_input])\n",
    "t_dense_output = t_model.get_layer('output')\n",
    "t_dec_prediction = t_dense_output(t_dec_output)\n",
    "t_dec_model = Model(inputs=[t_dec_input, t_dec_memory_state_input, t_dec_carry_state_input],\n",
    "                    outputs=[t_dec_prediction, t_dec_memory_state_output, t_dec_carry_state_output])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = ['captain rex', 'obi-wan kenobi']\n",
    "char2int = dict((c, i) for i, c in enumerate(vocab))\n",
    "\n",
    "vec_input_strings = process_input(strings, vocab)\n",
    "\n",
    "#end_char = np.zeros((len(vocab)), dtype=np.int8) # creates end of sentence character for the decoder\n",
    "#end_char[char2int['\\n']] = 1\n",
    "\n",
    "vec_sentences = np.zeros((0, MAX_LEN_TEXT, len(vocab)))\n",
    "for vec_input_string in vec_input_strings:\n",
    "\n",
    "    vec_input_string = np.expand_dims(vec_input_string, axis=0)\n",
    "    [memory_state, carry_state] = t_enc_model.predict([vec_input_string]) # get initial states generated by the encoder\n",
    "\n",
    "    one_hot_char_output = np.zeros((len(vocab)), dtype=np.int8) # creates initial character for the decoder\n",
    "    one_hot_char_output[char2int['[START]']] = 1\n",
    "    one_hot_char_output = np.expand_dims(one_hot_char_output, axis=0)\n",
    "    one_hot_char_output = np.expand_dims(one_hot_char_output, axis=0)\n",
    "\n",
    "    vec_sentence = np.empty((0, len(vocab)))\n",
    "    for _ in range(MAX_LEN_TEXT):\n",
    "\n",
    "        [one_hot_char_output, memory_state, carry_state] = t_dec_model.predict([one_hot_char_output, memory_state, carry_state])\n",
    "        vec_sentence = np.append(vec_sentence, one_hot_char_output[0], axis=0)\n",
    "\n",
    "    \n",
    "    vec_sentences = np.append(vec_sentences, [vec_sentence], axis=0)\n",
    "    \n",
    "strings = one_hot2char(vec_sentences, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['tapata  a was a wase wane  aa       was   was   was a tame out  t aaa  aa  a a a a a                                                      \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"  \"\"\"\"   \"\"\"  a \"\"\" was \" \"   a \"\"  aas    as )) was )  as )) was a mane- la    a   a a a a a a a a a a a                                             )))))))))))))wwff)) was a mader laae  a   a a a d a d d d d d d d ',\n 'the oh--d sesul l d s a a s a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a                                                         )))))))))))))wwwkk,, was a mader caae  aa   a a a a a a a d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d a ']"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "training_logs_1 = pd.read_csv('Training Logs/Training Session - 03-May-20 -- 04-00-16/Training logs.csv', index_col=0)\n",
    "\n",
    "training_logs_2 = pd.read_csv('Training Logs/Training Session - 04-May-20 -- 04-41-34/Training logs.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "           loss  val_loss\nepoch                    \n0      0.260818  0.176167\n1      0.190448  0.154953\n2      0.157459  0.137437\n3      0.146972  0.125717\n4      0.133152  0.122782\n5      0.126164  0.111172\n6      0.123544  0.105922\n7      0.115008  0.106538\n8      0.111435  0.099002\n9      0.111045  0.096033\n10     0.104601  0.097204\n11     0.101954  0.091663\n12     0.102682  0.089812\n13     0.097171  0.091324\n14     0.095351  0.086998\n15     0.096621  0.085768\n16     0.091958  0.088084\n17     0.090545  0.084032\n18     0.092405  0.083375\n19     0.088144  0.085468\n20     0.087052  0.082174\n21     0.089214  0.081663\n22     0.085257  0.083611\n23     0.084198  0.080673\n24     0.086560  0.080451\n25     0.083114  0.082218\n26     0.082136  0.079871\n27     0.084404  0.079498\n28     0.080972  0.081564\n29     0.080528  0.079126\n30     0.082864  0.078852\n31     0.079340  0.080936\n32     0.079216  0.078683\n33     0.081443  0.078352\n34     0.077862  0.080343\n35     0.078088  0.078092\n36     0.080224  0.077950\n37     0.077022  0.079891\n38     0.076997  0.077806\n39     0.079064  0.077679",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>val_loss</th>\n    </tr>\n    <tr>\n      <th>epoch</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.260818</td>\n      <td>0.176167</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.190448</td>\n      <td>0.154953</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.157459</td>\n      <td>0.137437</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.146972</td>\n      <td>0.125717</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.133152</td>\n      <td>0.122782</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.126164</td>\n      <td>0.111172</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.123544</td>\n      <td>0.105922</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.115008</td>\n      <td>0.106538</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.111435</td>\n      <td>0.099002</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.111045</td>\n      <td>0.096033</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.104601</td>\n      <td>0.097204</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.101954</td>\n      <td>0.091663</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.102682</td>\n      <td>0.089812</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.097171</td>\n      <td>0.091324</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.095351</td>\n      <td>0.086998</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.096621</td>\n      <td>0.085768</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.091958</td>\n      <td>0.088084</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.090545</td>\n      <td>0.084032</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.092405</td>\n      <td>0.083375</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.088144</td>\n      <td>0.085468</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.087052</td>\n      <td>0.082174</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.089214</td>\n      <td>0.081663</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.085257</td>\n      <td>0.083611</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.084198</td>\n      <td>0.080673</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.086560</td>\n      <td>0.080451</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.083114</td>\n      <td>0.082218</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.082136</td>\n      <td>0.079871</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.084404</td>\n      <td>0.079498</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.080972</td>\n      <td>0.081564</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.080528</td>\n      <td>0.079126</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.082864</td>\n      <td>0.078852</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.079340</td>\n      <td>0.080936</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.079216</td>\n      <td>0.078683</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.081443</td>\n      <td>0.078352</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.077862</td>\n      <td>0.080343</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.078088</td>\n      <td>0.078092</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0.080224</td>\n      <td>0.077950</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0.077022</td>\n      <td>0.079891</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0.076997</td>\n      <td>0.077806</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>0.079064</td>\n      <td>0.077679</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "training_logs_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "           loss  val_loss\nepoch                    \n0      0.304460  0.203179\n1      0.216608  0.188706\n2      0.191461  0.172928\n3      0.180170  0.156469\n4      0.161508  0.150988\n5      0.150069  0.133233\n6      0.144843  0.125357\n7      0.133633  0.124558\n8      0.128468  0.114726\n9      0.127640  0.113289\n10     0.122491  0.114025\n11     0.119252  0.106851\n12     0.119748  0.104439\n13     0.113901  0.106458\n14     0.111794  0.100707\n15     0.113238  0.098806\n16     0.108021  0.101278\n17     0.106472  0.096407\n18     0.108383  0.095531\n19     0.104032  0.097842\n20     0.102617  0.093727\n21     0.105024  0.092685\n22     0.100689  0.095018\n23     0.099545  0.091133\n24     0.102092  0.090483\n25     0.098055  0.093058\n26     0.097199  0.089439\n27     0.099638  0.088911\n28     0.096099  0.091281\n29     0.095272  0.087939\n30     0.097996  0.087776\n31     0.094260  0.090154\n32     0.093724  0.086897\n33     0.096509  0.086780\n34     0.093189  0.089235\n35     0.092731  0.086069\n36     0.095590  0.086022\n37     0.091817  0.088379\n38     0.091647  0.085307\n39     0.094390  0.085273",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>val_loss</th>\n    </tr>\n    <tr>\n      <th>epoch</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.304460</td>\n      <td>0.203179</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.216608</td>\n      <td>0.188706</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.191461</td>\n      <td>0.172928</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.180170</td>\n      <td>0.156469</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.161508</td>\n      <td>0.150988</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.150069</td>\n      <td>0.133233</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.144843</td>\n      <td>0.125357</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.133633</td>\n      <td>0.124558</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.128468</td>\n      <td>0.114726</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.127640</td>\n      <td>0.113289</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.122491</td>\n      <td>0.114025</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.119252</td>\n      <td>0.106851</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.119748</td>\n      <td>0.104439</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.113901</td>\n      <td>0.106458</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.111794</td>\n      <td>0.100707</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.113238</td>\n      <td>0.098806</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.108021</td>\n      <td>0.101278</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.106472</td>\n      <td>0.096407</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.108383</td>\n      <td>0.095531</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.104032</td>\n      <td>0.097842</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.102617</td>\n      <td>0.093727</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.105024</td>\n      <td>0.092685</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.100689</td>\n      <td>0.095018</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.099545</td>\n      <td>0.091133</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.102092</td>\n      <td>0.090483</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.098055</td>\n      <td>0.093058</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.097199</td>\n      <td>0.089439</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.099638</td>\n      <td>0.088911</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.096099</td>\n      <td>0.091281</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.095272</td>\n      <td>0.087939</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.097996</td>\n      <td>0.087776</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.094260</td>\n      <td>0.090154</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.093724</td>\n      <td>0.086897</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.096509</td>\n      <td>0.086780</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.093189</td>\n      <td>0.089235</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.092731</td>\n      <td>0.086069</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0.095590</td>\n      <td>0.086022</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0.091817</td>\n      <td>0.088379</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0.091647</td>\n      <td>0.085307</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>0.094390</td>\n      <td>0.085273</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "training_logs_2"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37764bitvenvwindowsvenvb5f78b3c7dfa470db2232f7376ff9749",
   "display_name": "Python 3.7.7 64-bit ('venv-windows': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}