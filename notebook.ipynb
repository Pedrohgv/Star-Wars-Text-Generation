{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Using matplotlib backend: TkAgg\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras import Input, Model\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "\n",
    "from callbacks import CallbackPlot, CallbackSaveLogs\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_processor import build_datasets\n",
    "from data_processor import one_hot2char, process_input\n",
    "\n",
    "%matplotlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "from shutil import copyfile\n",
    "from datetime import datetime\n",
    "\n",
    "MAX_LEN_TITLE = 128\n",
    "MAX_LEN_TEXT = 390\n",
    "\n",
    "# enable memory growth to be able to work with GPU\n",
    "GPU = tf.config.experimental.get_visible_devices('GPU')[0]\n",
    "tf.config.experimental.set_memory_growth(GPU, enable=True)\n",
    "\n",
    "# set tensorflow to work with float64\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "# the new line character (\\n) is the 'end of sentence', therefore there is no need to add a '[STOP]' character\n",
    "vocab = '%?!:;/86XQ()975Z43U\"201YqjVzNJKLHWPxFOEM\\'DIG-BR\\nACSTkv,.bygwfpmucdlhsnirotae '\n",
    "vocab = list(vocab) + ['[START]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {  # dictionary that contains the training set up. Will be saved as a JSON file\n",
    "    'DIM_VOCAB': len(vocab),\n",
    "    'DIM_LSTM_LAYER': 256,\n",
    "    'LEARNING_RATE': 0.01,\n",
    "    'BATCH_SIZE': 16,\n",
    "    'EPOCHS': 10,\n",
    "    'SEED': 1,\n",
    "    'GRAD_VAL_CLIP': 0.5,\n",
    "    'GRAD_NORM_CLIP': 1\n",
    "}\n",
    "tf.random.set_seed(config['SEED'])\n",
    "data = pd.read_csv('Complete Database.csv', index_col=0)\n",
    "\n",
    "config['STEPS_PER_EPOCH'] = int((data.shape[0] - 3000) / config['BATCH_SIZE'])\n",
    "config['VALIDATION_SAMPLES'] = int(data.shape[0]) - (config['STEPS_PER_EPOCH'] * config['BATCH_SIZE'])\n",
    "config['VALIDATION_STEPS'] = int(np.floor(config['VALIDATION_SAMPLES'] / config['BATCH_SIZE']))\n",
    "\n",
    "\"\"\" config['STEPS_PER_EPOCH'] = 5\n",
    "config['VALIDATION_SAMPLES'] = 100\n",
    "config['VALIDATION_STEPS'] = 5 \"\"\"\n",
    "\n",
    "training_dataset, validation_dataset = build_datasets(data, seed=config['SEED'], validation_samples=config['VALIDATION_SAMPLES'], batch=config['BATCH_SIZE'], vocab=vocab)\n",
    "\n",
    "folder_path = 'Training Logs/Training'  # creates folder to save traning logs\n",
    "if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "with open(folder_path + '/config.json', 'w') as json_file: # saves the training configuration as a JSON file\n",
    "    json.dump(config, json_file, indent=4)\n",
    "\n",
    "with open(folder_path + '/vocab.json', 'w')  as json_file: # saves the vocab used as a JSON file\n",
    "    json.dump(vocab, json_file, indent=4)\n",
    "\n",
    "\n",
    "if config['GRAD_VAL_CLIP']:\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=config['LEARNING_RATE'], clipvalue=config['GRAD_VAL_CLIP'], clipnorm=config['GRAD_NORM_CLIP'])\n",
    "else:\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=config['LEARNING_RATE'])\n",
    "\n",
    "validation_steps = int(config['VALIDATION_SAMPLES'] / config['BATCH_SIZE'])\n",
    "\n",
    "loss_plot_settings = {'variables': {'loss': 'Training loss',\n",
    "                                        'val_loss': 'Validation loss'},\n",
    "                          'title': 'Losses',\n",
    "                          'ylabel': 'Epoch Loss'}\n",
    "\n",
    "last_5_plot_settings = {'variables': {'loss': 'Training loss',\n",
    "                                        'val_loss': 'Validation loss'},\n",
    "                          'title': 'Losses',\n",
    "                          'ylabel': 'Epoch Loss',\n",
    "                          'last_epochs': 5}\n",
    "\n",
    "plot_callback = CallbackPlot(folder_path=folder_path,\n",
    "                            plots_settings=[loss_plot_settings, last_5_plot_settings],\n",
    "                            title='Losses', share_x=False)\n",
    "\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=folder_path + '/trained_model.h5')\n",
    "\n",
    "csv_logger = CSVLogger(filename=folder_path + '/Training logs.csv', separator=',', append=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### BUILDS MODEL FROM SCRATCH ###########\n",
    "tf.keras.backend.clear_session() # destroys the current graph\n",
    "\n",
    "encoder_inputs = Input(shape=(None, config['DIM_VOCAB']), name='encoder_input')\n",
    "encoder_LSTM = LSTM(units = config['DIM_LSTM_LAYER'], return_sequences=False, return_state=True, name='encoder_LSTM')\n",
    "enc_output, enc_memory_state, enc_carry_state = encoder_LSTM(encoder_inputs)\n",
    "\n",
    "decoder_inputs = Input(shape=(None, config['DIM_VOCAB']), name='decoder_input')\n",
    "decoder_LSTM = LSTM(units=config['DIM_LSTM_LAYER'], return_sequences=True, return_state=True, name='decoder_LSTM')  # return_state must be set in order to retrieve the internal states in inference model later\n",
    "decoder_output, _, _ = decoder_LSTM(decoder_inputs, initial_state=[enc_memory_state, enc_carry_state])\n",
    "\n",
    "dense = Dense(units=config['DIM_VOCAB'], activation='softmax', name='output')\n",
    "dense_output = dense(decoder_output)\n",
    "\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=dense_output)\n",
    "\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### LOADS MODEL FROM DISK ###########\n",
    "\n",
    "tf.keras.backend.clear_session() # destroys the current graph\n",
    "\n",
    "model_folder = 'Training Logs/Training Session - 23-Apr-20 -- 14-39-36'\n",
    "\n",
    "model = tf.keras.models.load_model(model_folder + '/trained_model.h5')\n",
    "\n",
    "copyfile(model_folder + '/Training logs.csv', folder_path + '/Training logs.csv') # makes a copy of the logs from previous training to the destination folder\n",
    "\n",
    "csv_logger = CSVLogger(filename=folder_path + '/Training logs.csv', separator=',', append=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train for 1126 steps, validate for 187 steps\nEpoch 1/10\n1126/1126 [==============================] - 239s 212ms/step - loss: 0.1874 - val_loss: 0.1210\nEpoch 2/10\n1126/1126 [==============================] - 244s 217ms/step - loss: 0.2192 - val_loss: 0.1894\nEpoch 3/10\n1126/1126 [==============================] - 246s 218ms/step - loss: 0.1712 - val_loss: 0.1587\nEpoch 4/10\n  54/1126 [>.............................] - ETA: 3:39 - loss: 0.1746"
    }
   ],
   "source": [
    "\n",
    "history = model.fit(x=training_dataset,\n",
    "                    epochs=config['EPOCHS'],\n",
    "                    steps_per_epoch=config['STEPS_PER_EPOCH'],\n",
    "                    callbacks=[plot_callback, csv_logger, model_checkpoint_callback],\n",
    "                    validation_data=validation_dataset,\n",
    "                    validation_steps=config['VALIDATION_STEPS'])\n",
    "                    \n",
    "\n",
    "'''\n",
    "model.save(folder_path + '/trained_model.h5', save_format='h5')\n",
    "model.save_weights(folder_path + '/trained_model_weights.h5')\n",
    "plot_model(model, to_file=folder_path + '/model_layout.png', show_shapes=True, show_layer_names=True, rankdir='LR')\n",
    "'''\n",
    "\n",
    "timestamp_end = datetime.now().strftime('%d-%b-%y -- %H:%M:%S')\n",
    "\n",
    "# renames the training folder with the end-of-training timestamp\n",
    "root, _ = os.path.split(folder_path)\n",
    "\n",
    "timestamp_end = timestamp_end.replace(':', '-')\n",
    "os.rename(folder_path, root + '/' + 'Training Session - ' + timestamp_end)\n",
    "\n",
    "print(\"Training Succesfully finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a dataframe that contains all the characters and their count in all the texts and titles\n",
    "concatenated_text = data.Text.str.cat() + data.Title.str.cat()\n",
    "vocab = set(list(concatenated_text))\n",
    "\n",
    "char_count = []\n",
    "for char in vocab:\n",
    "    char_count.append([char, concatenated_text.count(char)])\n",
    "\n",
    "df_count = pd.DataFrame(char_count, columns=['Char', 'Count']) # must be 'Count' with capital 'c' to avoid conflict with function 'count' form pandas\n",
    "df_count.set_index('Char', inplace=True)\n",
    "df_count.sort_values(by='Count', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = 'Training Logs/Training Session - 26-Apr-20 -- 06-52-30'\n",
    "strings = ['Obi-Wan Kenobi']\n",
    "\n",
    "# loads trained model from H5 file. 't' stands for 'trained'\n",
    "t_model = tf.keras.models.load_model(model_folder + '/trained_model.h5')\n",
    "\n",
    "# loads model configuration from JSON file\n",
    "with open(model_folder + '/config.json') as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "# loads vocab from JSON file\n",
    "with open(model_folder + '/vocab.json') as json_file:\n",
    "    vocab = json.load(json_file)\n",
    "\n",
    "# encoder model. 't' stands for 'trained'\n",
    "t_enc_input = t_model.get_layer('encoder_input').input\n",
    "t_enc_LSTM = t_model.get_layer('encoder_LSTM')\n",
    "[_, t_enc_memory_state, t_enc_carry_state] = t_enc_LSTM(t_enc_input)\n",
    "t_enc_model = Model(inputs=t_enc_input,\n",
    "                    outputs=[t_enc_memory_state, t_enc_carry_state])\n",
    "\n",
    "# decoder inputs\n",
    "t_dec_input = Input(shape=(None, config['DIM_VOCAB']), name='decoder_input')\n",
    "t_dec_memory_state_input = Input(shape=(config['DIM_LSTM_LAYER']), name='decoder_memory_state_input')\n",
    "t_dec_carry_state_input = Input(shape=(config['DIM_LSTM_LAYER']), name='decoder_carry_state_input')\n",
    "\n",
    "# decoder model\n",
    "t_dec_LSTM = t_model.get_layer('decoder_LSTM')\n",
    "t_dec_output, t_dec_memory_state_output, t_dec_carry_state_output = t_dec_LSTM(t_dec_input, initial_state=[t_dec_memory_state_input, t_dec_carry_state_input])\n",
    "t_dense_output = t_model.get_layer('output')\n",
    "t_dec_prediction = t_dense_output(t_dec_output)\n",
    "t_dec_model = Model(inputs=[t_dec_input, t_dec_memory_state_input, t_dec_carry_state_input],\n",
    "                    outputs=[t_dec_prediction, t_dec_memory_state_output, t_dec_carry_state_output])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2int = dict((c, i) for i, c in enumerate(vocab))\n",
    "\n",
    "vec_input_strings = process_input(strings, vocab)\n",
    "\n",
    "end_char = np.zeros((len(vocab)), dtype=np.int8) # creates end of sentence character for the decoder\n",
    "end_char[char2int['\\n']] = 1\n",
    "\n",
    "vec_sentences = np.zeros((0, MAX_LEN_TEXT, len(vocab)))\n",
    "for vec_input_string in vec_input_strings:\n",
    "\n",
    "    vec_input_string = np.expand_dims(vec_input_string, axis=0)\n",
    "    [memory_state, carry_state] = t_enc_model.predict([vec_input_string]) # get initial states generated by the encoder\n",
    "\n",
    "    one_hot_char_output = np.zeros((len(vocab)), dtype=np.int8) # creates initial character for the decoder\n",
    "    one_hot_char_output[char2int['[START]']] = 1\n",
    "    one_hot_char_output = np.expand_dims(one_hot_char_output, axis=0)\n",
    "    one_hot_char_output = np.expand_dims(one_hot_char_output, axis=0)\n",
    "\n",
    "    vec_sentence = np.empty((0, len(vocab)))\n",
    "    for _ in range(MAX_LEN_TEXT):\n",
    "\n",
    "        [one_hot_char_output, memory_state, carry_state] = t_dec_model.predict([one_hot_char_output, memory_state, carry_state])\n",
    "        vec_sentence = np.append(vec_sentence, one_hot_char_output[0], axis=0)\n",
    "\n",
    "    \n",
    "    vec_sentences = np.append(vec_sentences, [vec_sentence], axis=0)\n",
    "    \n",
    "strings = one_hot2char(vec_sentences, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processor import clean_data\n",
    "df = pd.read_csv('Complete Database.csv', index_col=0)\n",
    "df = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trimed_text'] = df.Text.str.split('.').map(lambda x: x[0] + '.\\n')\n",
    "df['trimed_text_len'] = df['trimed_text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Text[df.Text.str.len() == 389]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.trimed_text_len[df.trimed_text_len.between(340,400)].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "\n",
    "print(df[df.trimed_text_len > 350].index[index])\n",
    "print(df.Title[df.trimed_text_len > 350].iloc[index])\n",
    "print(df.Text[df.trimed_text_len > 350].iloc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Text.iloc[1352] = 'The Attack on Chandrila occurred during the festivities of Liberation Day—the day scheduled for peace talks between Mon Mothma of the New Republic and Rae Sloane of the Galactic Empire—on Chandrila.\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Text.iloc[1352]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bitvenvlinuxvirtualenv279fb5b12a93435da082ad27595570f1",
   "display_name": "Python 3.6.9 64-bit ('venv-linux': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}