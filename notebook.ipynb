{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Using matplotlib backend: TkAgg\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras import Input, Model\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "from callbacks import CallbackPlot, CallbackSaveLogs\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_processor import clean_data, build_datasets\n",
    "from data_processor import one_hot2char, process_input\n",
    "\n",
    "%matplotlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "from shutil import copyfile\n",
    "from datetime import datetime\n",
    "\n",
    "MAX_LEN_TITLE = 67\n",
    "MAX_LEN_TEXT = 120\n",
    "\n",
    "# enable memory growth to be able to work with GPU\n",
    "GPU = tf.config.experimental.get_visible_devices('GPU')[0]\n",
    "tf.config.experimental.set_memory_growth(GPU, enable=True)\n",
    "\n",
    "# set tensorflow to work with float64\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "# the new line character (\\n) is the 'end of sentence', therefore there is no need to add a '[STOP]' character\n",
    "vocab = 'c-y5i8\"j\\'fk,theqm:/.wnlrdg0u1 v\\n4b97)o36z2axs(p'\n",
    "vocab = list(vocab) + ['[START]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Using gradient clipping\n"
    }
   ],
   "source": [
    "\n",
    "config = {  # dictionary that contains the training set up. Will be saved as a JSON file\n",
    "    'DIM_VOCAB': len(vocab),\n",
    "    'DIM_LSTM_LAYER': 512,\n",
    "    'ENCODER_DEPTH': 2,\n",
    "    'DECODER_DEPTH': 2,\n",
    "    'LEARNING_RATE': 0.001,\n",
    "    'BATCH_SIZE': 32,\n",
    "    'EPOCHS': 40,\n",
    "    'SEED': 1,\n",
    "    'GRAD_VAL_CLIP': 0.5,\n",
    "    'GRAD_NORM_CLIP': 1,\n",
    "    'DECAY_AT_10_EPOCHS': 1,\n",
    "    'DROPOUT': 0.2,\n",
    "}\n",
    "tf.random.set_seed(config['SEED'])\n",
    "data = pd.read_csv('Complete Database.csv', index_col=0)\n",
    "\n",
    "data = clean_data(data)\n",
    "\n",
    "config['STEPS_PER_EPOCH'] = int((data.shape[0] - 3000) / config['BATCH_SIZE'])\n",
    "config['VALIDATION_SAMPLES'] = int(data.shape[0]) - (config['STEPS_PER_EPOCH'] * config['BATCH_SIZE'])\n",
    "config['VALIDATION_STEPS'] = int(np.floor(config['VALIDATION_SAMPLES'] / config['BATCH_SIZE']))\n",
    "'''\n",
    "config['STEPS_PER_EPOCH'] = 5\n",
    "config['VALIDATION_SAMPLES'] = 100\n",
    "config['VALIDATION_STEPS'] = 5\n",
    "'''\n",
    "# configures the learning rate to be decayed by the value specified at config['DECAY_AT_10_EPOCHS'] at each 10 epochs, but to that gradally at each epoch\n",
    "learning_rate = ExponentialDecay(initial_learning_rate=config['LEARNING_RATE'], \n",
    "                                decay_steps=config['STEPS_PER_EPOCH'], \n",
    "                                decay_rate=np.power(config['DECAY_AT_10_EPOCHS'], 1/10), \n",
    "                                staircase=True)\n",
    "\n",
    "\n",
    " \n",
    "training_dataset, validation_dataset = build_datasets(data, seed=config['SEED'], validation_samples=config['VALIDATION_SAMPLES'], batch=config['BATCH_SIZE'], vocab=vocab)\n",
    "\n",
    "#learning_rate = config['LEARNING_RATE']\n",
    "\n",
    "if 'GRAD_VAL_CLIP' in config:\n",
    "    optimizer = Adam(learning_rate=learning_rate, clipvalue=config['GRAD_VAL_CLIP'], clipnorm=config['GRAD_NORM_CLIP'])\n",
    "    print('Using gradient clipping')\n",
    "else:\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "config['OPTIMIZER'] = optimizer._name\n",
    "\n",
    "folder_path = 'Training Logs/Training'  # creates folder to save traning logs\n",
    "if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "with open(folder_path + '/config.json', 'w') as json_file: # saves the training configuration as a JSON file\n",
    "    json.dump(config, json_file, indent=4)\n",
    "\n",
    "with open(folder_path + '/vocab.json', 'w')  as json_file: # saves the vocab used as a JSON file\n",
    "    json.dump(vocab, json_file, indent=4)\n",
    "\n",
    "\n",
    "validation_steps = int(config['VALIDATION_SAMPLES'] / config['BATCH_SIZE'])\n",
    "\n",
    "loss_plot_settings = {'variables': {'loss': 'Training loss',\n",
    "                                        'val_loss': 'Validation loss'},\n",
    "                          'title': 'Losses',\n",
    "                          'ylabel': 'Epoch Loss'}\n",
    "\n",
    "last_5_plot_settings = {'variables': {'loss': 'Training loss',\n",
    "                                        'val_loss': 'Validation loss'},\n",
    "                          'title': 'Losses',\n",
    "                          'ylabel': 'Epoch Loss',\n",
    "                          'last_epochs': 5}\n",
    "\n",
    "plot_callback = CallbackPlot(folder_path=folder_path,\n",
    "                            plots_settings=[loss_plot_settings, last_5_plot_settings],\n",
    "                            title='Losses', share_x=False)\n",
    "\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=folder_path + '/trained_model.h5')\n",
    "\n",
    "csv_logger = CSVLogger(filename=folder_path + '/Training logs.csv', separator=',', append=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### BUILDS MODEL FROM SCRATCH WITH MULTI LAYER LSTM ###########\n",
    "tf.keras.backend.clear_session() # destroys the current graph\n",
    "\n",
    "encoder_inputs = Input(shape=(None, config['DIM_VOCAB']), name='encoder_input')\n",
    "enc_internal_tensor = encoder_inputs\n",
    "\n",
    "for i in range(config['ENCODER_DEPTH']):\n",
    "    encoder_LSTM = LSTM(units=config['DIM_LSTM_LAYER'], return_sequences=True, return_state=True, name='encoder_LSTM_' + str(i), dropout=config['DROPOUT'])\n",
    "    enc_internal_tensor, enc_memory_state, enc_carry_state = encoder_LSTM(enc_internal_tensor) # only the last states are of interest\n",
    "\n",
    "decoder_inputs = Input(shape=(None, config['DIM_VOCAB']), name='decoder_input')\n",
    "dec_internal_tensor = decoder_inputs\n",
    "\n",
    "for i in range(config['DECODER_DEPTH']):\n",
    "    decoder_LSTM = LSTM(units=config['DIM_LSTM_LAYER'], return_sequences=True, return_state=True, name='decoder_LSTM_' + str(i), dropout=config['DROPOUT'])  # return_state must be set in order to retrieve the internal states in inference model later\n",
    "    # every LSTM layer in the decoder model have their states initialized with states from last time step from last LSTM layer in the encoder\n",
    "    dec_internal_tensor, _, _ = decoder_LSTM(dec_internal_tensor, initial_state=[enc_memory_state, enc_carry_state]) \n",
    "\n",
    "decoder_output = dec_internal_tensor\n",
    "\n",
    "dense = Dense(units=config['DIM_VOCAB'], activation='softmax', name='output')\n",
    "dense_output = dense(decoder_output)\n",
    "\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=dense_output)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train for 562 steps, validate for 94 steps\nEpoch 1/40\n562/562 [==============================] - 1548s 3s/step - loss: 0.2993 - val_loss: 0.2419\nEpoch 2/40\n562/562 [==============================] - 1528s 3s/step - loss: 0.2233 - val_loss: 0.1879\nEpoch 3/40\n562/562 [==============================] - 1528s 3s/step - loss: 0.1966 - val_loss: 0.1663\nEpoch 4/40\n562/562 [==============================] - 1528s 3s/step - loss: 0.1790 - val_loss: 0.1443\nEpoch 5/40\n562/562 [==============================] - 1528s 3s/step - loss: 0.1635 - val_loss: 0.1415\nEpoch 6/40\n562/562 [==============================] - 1528s 3s/step - loss: 0.1534 - val_loss: 0.1258\nEpoch 7/40\n562/562 [==============================] - 1528s 3s/step - loss: 0.1480 - val_loss: 0.1169\nEpoch 8/40\n562/562 [==============================] - 1529s 3s/step - loss: 0.1418 - val_loss: 0.1201\nEpoch 9/40\n562/562 [==============================] - 1529s 3s/step - loss: 0.1363 - val_loss: 0.1112\nEpoch 10/40\n562/562 [==============================] - 1529s 3s/step - loss: 0.1368 - val_loss: 0.1085\nEpoch 11/40\n562/562 [==============================] - 1529s 3s/step - loss: 0.1313 - val_loss: 0.1101\nEpoch 12/40\n562/562 [==============================] - 1529s 3s/step - loss: 0.1273 - val_loss: 0.1035\nEpoch 13/40\n562/562 [==============================] - 1529s 3s/step - loss: 0.1268 - val_loss: 0.0992\nEpoch 14/40\n562/562 [==============================] - 1529s 3s/step - loss: 0.1240 - val_loss: 0.1042\nEpoch 15/40\n562/562 [==============================] - 1530s 3s/step - loss: 0.1220 - val_loss: 0.0984\nEpoch 16/40\n562/562 [==============================] - 1529s 3s/step - loss: 0.1221 - val_loss: 0.0950\nEpoch 17/40\n562/562 [==============================] - 1529s 3s/step - loss: 0.1191 - val_loss: 0.1001\nEpoch 18/40\n562/562 [==============================] - 1529s 3s/step - loss: 0.1174 - val_loss: 0.0946\nEpoch 19/40\n562/562 [==============================] - 1529s 3s/step - loss: 0.1175 - val_loss: 0.0918\nEpoch 20/40\n562/562 [==============================] - 1529s 3s/step - loss: 0.1149 - val_loss: 0.0961\nEpoch 21/40\n562/562 [==============================] - 1529s 3s/step - loss: 0.1138 - val_loss: 0.0919\nEpoch 22/40\n562/562 [==============================] - 1544s 3s/step - loss: 0.1137 - val_loss: 0.0888\nEpoch 23/40\n120/562 [=====>........................] - ETA: 21:32 - loss: 0.1143"
    }
   ],
   "source": [
    "\n",
    "history = model.fit(x=training_dataset,\n",
    "                    epochs=config['EPOCHS'],\n",
    "                    steps_per_epoch=config['STEPS_PER_EPOCH'],\n",
    "                    callbacks=[plot_callback, csv_logger, model_checkpoint_callback],\n",
    "                    validation_data=validation_dataset,\n",
    "                    validation_steps=config['VALIDATION_STEPS'])\n",
    "                   \n",
    "\n",
    "'''\n",
    "model.save(folder_path + '/trained_model.h5', save_format='h5')\n",
    "model.save_weights(folder_path + '/trained_model_weights.h5')\n",
    "plot_model(model, to_file=folder_path + '/model_layout.png', show_shapes=True, show_layer_names=True, rankdir='LR')\n",
    "'''\n",
    "\n",
    "timestamp_end = datetime.now().strftime('%d-%b-%y -- %H:%M:%S')\n",
    "\n",
    "# renames the training folder with the end-of-training timestamp\n",
    "root, _ = os.path.split(folder_path)\n",
    "\n",
    "timestamp_end = timestamp_end.replace(':', '-')\n",
    "os.rename(folder_path, root + '/' + 'Training Session - ' + timestamp_end)\n",
    "\n",
    "print(\"Training Succesfully finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a dataframe that contains all the characters and their count in all the texts and titles\n",
    "import pandas as pd\n",
    "from data_processor import clean_data\n",
    "\n",
    "data = pd.read_csv('Complete Database.csv', index_col=0)\n",
    "\n",
    "data = clean_data(data)\n",
    "\n",
    "concatenated_text = data.Text.str.cat() + data.Title.str.cat()\n",
    "vocab = set(list(concatenated_text))\n",
    "\n",
    "char_count = []\n",
    "for char in vocab:\n",
    "    char_count.append([char, concatenated_text.count(char)])\n",
    "\n",
    "df_count = pd.DataFrame(char_count, columns=['Char', 'Count']) # must be 'Count' with capital 'c' to avoid conflict with function 'count' form pandas\n",
    "df_count.set_index('Char', inplace=True)\n",
    "df_count.sort_values(by='Count', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "\n",
    "model_folder1 = 'Training Logs/Training Session - 03-May-20 -- 04-00-16'\n",
    "model_folder2 = 'Training Logs/Training Session - 04-May-20 -- 04-41-34'\n",
    "with open(model_folder1 + '/config.json') as json_file:\n",
    "    config1 = json.load(json_file)\n",
    "with open(model_folder2 + '/config.json') as json_file:\n",
    "    config2 = json.load(json_file)\n",
    "\n",
    "config1 == config2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### MULTI LSTM LAYER LOADER ############################\n",
    "\n",
    "model_folder = 'Training Logs/Training Session - 17-May-20 -- 11-42-37'\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session() # destroys the current graph\n",
    "\n",
    "# loads trained model from H5 file. 't' stands for 'trained'\n",
    "t_model = tf.keras.models.load_model(model_folder + '/trained_model.h5')\n",
    "\n",
    "# loads model configuration from JSON file\n",
    "with open(model_folder + '/config.json') as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "# loads vocab from JSON file\n",
    "with open(model_folder + '/vocab.json') as json_file:\n",
    "    vocab = json.load(json_file)\n",
    "\n",
    "# encoder model. 't' stands for 'trained'\n",
    "t_enc_input = t_model.get_layer('encoder_input').input\n",
    "t_enc_internal_tensor = t_enc_input\n",
    "\n",
    "for i in range(config['ENCODER_DEPTH']):\n",
    "    t_enc_LSTM = t_model.get_layer('encoder_LSTM_' + str(i))\n",
    "    [t_enc_internal_tensor, t_enc_memory_state, t_enc_carry_state] = t_enc_LSTM(t_enc_internal_tensor)\n",
    "\n",
    "\n",
    "t_enc_model = Model(inputs=t_enc_input,\n",
    "                    outputs=[t_enc_memory_state, t_enc_carry_state])\n",
    "\n",
    "# decoder inputs\n",
    "# the model needs to have an initial input, and inputs for every state (both memory and carry states) for each LSTM layer in the decoder,\n",
    "# because it decodes one time step at a time, always using the previous prediction as input\n",
    "t_dec_input = Input(shape=(None, config['DIM_VOCAB']), name='decoder_input')\n",
    "t_dec_internal_tensor = t_dec_input\n",
    "\n",
    "t_dec_memory_state_inputs = []   # list of all input memory state tensors (1 per LSTM layer)\n",
    "t_dec_carry_state_inputs = []  # list of all input carry state tensors (1 per LSTM layer)\n",
    "\n",
    "for i in range(config['DECODER_DEPTH']):\n",
    "    t_dec_memory_state_inputs.append(Input(shape=(config['DIM_LSTM_LAYER']), name='decoder_memory_state_input_' + str(i)))\n",
    "    t_dec_carry_state_inputs.append(Input(shape=(config['DIM_LSTM_LAYER']), name='decoder_carry_state_input_' + str(i)))\n",
    "\n",
    "t_dec_memory_state_outputs = [] # list of all output memory state tensors (1 per LSTM layer)\n",
    "t_dec_carry_state_outputs = [] # list of all output carry state tensors (1 per LSTM layer)\n",
    "\n",
    "# decoder model\n",
    "for i in range(config['DECODER_DEPTH']):\n",
    "    t_dec_LSTM = t_model.get_layer('decoder_LSTM_' + str(i))\n",
    "    t_dec_internal_tensor, t_dec_memory_state_output, t_dec_carry_state_output = t_dec_LSTM(t_dec_internal_tensor, initial_state=[t_dec_memory_state_inputs[i], t_dec_carry_state_inputs[i]])\n",
    "    t_dec_memory_state_outputs.append(t_dec_memory_state_output)\n",
    "    t_dec_carry_state_outputs.append(t_dec_carry_state_output)\n",
    "\n",
    "t_dec_output = t_dec_internal_tensor # output of last LSTM layer\n",
    "\n",
    "t_dense_output = t_model.get_layer('output')\n",
    "t_dec_prediction = t_dense_output(t_dec_output)\n",
    "t_dec_model = Model(inputs=[t_dec_input] + t_dec_memory_state_inputs + t_dec_carry_state_inputs,\n",
    "                    outputs=[t_dec_prediction] + t_dec_memory_state_outputs + t_dec_carry_state_outputs)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = ['captain rex', 'obi-wan kenobi']\n",
    "char2int = dict((c, i) for i, c in enumerate(vocab))\n",
    "\n",
    "vec_input_strings = process_input(strings, vocab)\n",
    "\n",
    "#end_char = np.zeros((len(vocab)), dtype=np.int8) # creates end of sentence character for the decoder\n",
    "#end_char[char2int['\\n']] = 1\n",
    "\n",
    "vec_sentences = np.zeros((0, MAX_LEN_TEXT, len(vocab)))\n",
    "for vec_input_string in vec_input_strings:\n",
    "\n",
    "    vec_input_string = np.expand_dims(vec_input_string, axis=0)\n",
    "    [enc_memory_state, enc_carry_state] = t_enc_model.predict([vec_input_string]) # get initial states generated by the encoder\n",
    "\n",
    "    one_hot_char_output = np.zeros((len(vocab)), dtype=np.int8) # creates initial character for the decoder\n",
    "    one_hot_char_output[char2int['[START]']] = 1\n",
    "    one_hot_char_output = np.expand_dims(one_hot_char_output, axis=0)\n",
    "    one_hot_char_output = np.expand_dims(one_hot_char_output, axis=0)\n",
    "\n",
    "    memory_states = []  # list of all input memory states (1 per LSTM layer)\n",
    "    carry_states = []   # list of all input carry states (1 per LSTM layer)\n",
    "\n",
    "    for i in range(config['DECODER_DEPTH']):    # initializes all memory and carry states with states from last layer of the encoder\n",
    "        memory_states.append(enc_memory_state)\n",
    "        carry_states.append(enc_carry_state)\n",
    "\n",
    "    vec_sentence = np.empty((0, len(vocab)))\n",
    "\n",
    "    for _ in range(MAX_LEN_TEXT):\n",
    "\n",
    "        outputs = t_dec_model.predict([one_hot_char_output] + memory_states + carry_states) # output contains the output and each layer's memory and carry states\n",
    "        one_hot_char_output = outputs.pop(0) # separates the output\n",
    "        memory_states = outputs[: len(outputs)//2]  # separates memory states from all layers\n",
    "        carry_states = outputs[len(outputs)//2 :]   # separates carry states from all layers\n",
    "        vec_sentence = np.append(vec_sentence, one_hot_char_output[0], axis=0)\n",
    "        \n",
    "    \n",
    "    vec_sentences = np.append(vec_sentences, [vec_sentence], axis=0)\n",
    "    \n",
    "strings = one_hot2char(vec_sentences, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[\"captain iis was a malier male who served as a cirettin aomatel in the galen system.\\n''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\",\n \"the obinite relter was a trae of eooitaled teviu tave toot toot bone ated by the fane oange s oart o  \\n'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\"]"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "training_logs_1 = pd.read_csv('Training Logs/Training/Training logs.csv', index_col=0)\n",
    "\n",
    "training_logs_2 = pd.read_csv('Training Logs/Training Session - 04-May-20 -- 04-41-34/Training logs.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "           loss  val_loss\nepoch                    \n0      0.238608  0.153786\n1      0.150679  0.120578\n2      0.128761  0.113731\n3      0.121985  0.101752\n4      0.113408  0.096178\n5      0.111345  0.098579\n6      0.108054  0.091458\n7      0.102975  0.088828\n8      0.100700  0.091857\n9      0.101815  0.087456\n10     0.097550  0.085116\n11     0.097193  0.091030\n12     0.097425  0.083063\n13     0.092200  0.081313\n14     0.090638  0.083681\n15     0.092562  0.080612\n16     0.089377  0.079169\n17     0.088104  0.082053\n18     0.090083  0.078827\n19     0.086518  0.077378\n20     0.085148  0.079863\n21     0.086954  0.077450\n22     0.084376  0.075681\n23     0.083258  0.078513",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>val_loss</th>\n    </tr>\n    <tr>\n      <th>epoch</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.238608</td>\n      <td>0.153786</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.150679</td>\n      <td>0.120578</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.128761</td>\n      <td>0.113731</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.121985</td>\n      <td>0.101752</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.113408</td>\n      <td>0.096178</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.111345</td>\n      <td>0.098579</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.108054</td>\n      <td>0.091458</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.102975</td>\n      <td>0.088828</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.100700</td>\n      <td>0.091857</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.101815</td>\n      <td>0.087456</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.097550</td>\n      <td>0.085116</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.097193</td>\n      <td>0.091030</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.097425</td>\n      <td>0.083063</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.092200</td>\n      <td>0.081313</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.090638</td>\n      <td>0.083681</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.092562</td>\n      <td>0.080612</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.089377</td>\n      <td>0.079169</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.088104</td>\n      <td>0.082053</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.090083</td>\n      <td>0.078827</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.086518</td>\n      <td>0.077378</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.085148</td>\n      <td>0.079863</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.086954</td>\n      <td>0.077450</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.084376</td>\n      <td>0.075681</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.083258</td>\n      <td>0.078513</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "training_logs_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "folder_path = 'Training Logs/Training'\n",
    "\n",
    "model = load_model(folder_path + 'trained_model.h5')\n",
    "\n",
    "with open(model_folder1 + '/config.json') as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "tf.random.set_seed(config['SEED'])\n",
    "data = pd.read_csv('Complete Database.csv', index_col=0)\n",
    "\n",
    "data = clean_data(data)\n",
    "\n",
    "training_dataset, validation_dataset = build_datasets(data, seed=config['SEED'], validation_samples=config['VALIDATION_SAMPLES'], batch=config['BATCH_SIZE'], vocab=vocab)\n",
    "\n",
    "history = model.fit(x=training_dataset,\n",
    "                    epochs=config['EPOCHS'],\n",
    "                    steps_per_epoch=config['STEPS_PER_EPOCH'],\n",
    "                    callbacks=[plot_callback, csv_logger, model_checkpoint_callback],\n",
    "                    validation_data=validation_dataset,\n",
    "                    validation_steps=config['VALIDATION_STEPS'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tf'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d7ff2f680815>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tf'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Sequence to sequence example in Keras (character-level).\n",
    "\n",
    "This script demonstrates how to implement a basic character-level\n",
    "sequence-to-sequence model. We apply it to translating\n",
    "short English sentences into short French sentences,\n",
    "character-by-character. Note that it is fairly unusual to\n",
    "do character-level machine translation, as word-level\n",
    "models are more common in this domain.\n",
    "\n",
    "**Summary of the algorithm**\n",
    "\n",
    "- We start with input sequences from a domain (e.g. English sentences)\n",
    "    and corresponding target sequences from another domain\n",
    "    (e.g. French sentences).\n",
    "- An encoder LSTM turns input sequences to 2 state vectors\n",
    "    (we keep the last LSTM state and discard the outputs).\n",
    "- A decoder LSTM is trained to turn the target sequences into\n",
    "    the same sequence but offset by one timestep in the future,\n",
    "    a training process called \"teacher forcing\" in this context.\n",
    "    It uses as initial state the state vectors from the encoder.\n",
    "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
    "    given `targets[...t]`, conditioned on the input sequence.\n",
    "- In inference mode, when we want to decode unknown input sequences, we:\n",
    "    - Encode the input sequence into state vectors\n",
    "    - Start with a target sequence of size 1\n",
    "        (just the start-of-sequence character)\n",
    "    - Feed the state vectors and 1-char target sequence\n",
    "        to the decoder to produce predictions for the next character\n",
    "    - Sample the next character using these predictions\n",
    "        (we simply use argmax).\n",
    "    - Append the sampled character to the target sequence\n",
    "    - Repeat until we generate the end-of-sequence character or we\n",
    "        hit the character limit.\n",
    "\n",
    "**Data download**\n",
    "\n",
    "[English to French sentence pairs.\n",
    "](http://www.manythings.org/anki/fra-eng.zip)\n",
    "\n",
    "[Lots of neat sentence pairs datasets.\n",
    "](http://www.manythings.org/anki/)\n",
    "\n",
    "**References**\n",
    "\n",
    "- [Sequence to Sequence Learning with Neural Networks\n",
    "   ](https://arxiv.org/abs/1409.3215)\n",
    "- [Learning Phrase Representations using\n",
    "    RNN Encoder-Decoder for Statistical Machine Translation\n",
    "    ](https://arxiv.org/abs/1406.1078)\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'fra.txt'\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')\n",
    "\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Complete Database.csv', index_col=0)\n",
    "\n",
    "data = clean_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "67"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "data.Title.str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(11088, 3)"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "data.shape"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bitvenvlinuxvirtualenv405242cc04cd47c48095de5e043fb25b",
   "display_name": "Python 3.6.9 64-bit ('venv-linux': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}